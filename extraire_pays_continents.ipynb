{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def ouvrir_json(chemin):\n",
    "  f = open(chemin, encoding=\"utf-8\")\n",
    "  toto = json.load(f)\n",
    "  f.close()\n",
    "  return toto\n",
    "\n",
    "def ecrire_json(chemin, contenu):\n",
    "  w = open(chemin, \"w\", encoding=\"utf-8\")\n",
    "  w.write(json.dumps(contenu, indent=2, ensure_ascii=False))\n",
    "  w.close()\n",
    "\n",
    "import glob\n",
    "def lire_fichier(chemin):\n",
    "    f=open(chemin, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    chaine = f.read()\n",
    "    f.close()\n",
    "    return chaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a13f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern-matching pour trouver les noms de pays présents dans les 1000ers caractères des articles et les associer à leur continent\n",
    "countries = ouvrir_json(\"countries_continents.json\")\n",
    "dico_continent = {}\n",
    "for chemin in liste_articles:\n",
    "    article = lire_fichier(chemin)\n",
    "    chemin = chemin.split(\"/\")[-1]\n",
    "    chemin = chemin.split(\"\\\\\")[-1]\n",
    "    debut = article[:1000] #500\n",
    "    for iso, dico in countries.items():\n",
    "        if dico[\"name\"] in debut:\n",
    "            if chemin not in dico_continent:\n",
    "                dico_continent[chemin]={}\n",
    "            dico_continent[chemin][dico[\"name\"]]=dico[\"continent\"]\n",
    "\n",
    "print(len(dico_continent))\n",
    "print(dico_continent)\n",
    "\n",
    "#trouver les 3000 articles restants, non détectés avec la méthode précédente \n",
    "    #=> surtout Etats-Unis donc on essaye de trouver d'autres mots clefs pour les trouver\n",
    "indices_eu = [\".edu\", \"Google\",\"Facebook\",\"Microsoft\",\"California\",\"Washington\",\"Stanford\",\"Harvard\",\"Amazon\",\"Pennsylvania\",\"Brown University\",\"New York\"]\n",
    "#aussi utiliser les .de, .uk etc des mails donc une regex pour détecter les adresses mails (contenant @) puis -1 d'un split \".\"\n",
    "for chemin in liste_articles:\n",
    "    article = lire_fichier(chemin)\n",
    "    chemin = chemin.split(\"/\")[-1]\n",
    "    chemin = chemin.split(\"\\\\\")[-1]\n",
    "    debut = article[:1000]\n",
    "    if chemin not in dico_continent.keys():\n",
    "        for ind in indices_eu:\n",
    "            if ind in debut:\n",
    "                if chemin not in dico_continent:\n",
    "                    dico_continent[chemin]={}\n",
    "                dico_continent[chemin][dico[\"name\"]]=\"NA\"\n",
    "print(len(dico_continent))\n",
    "\n",
    "#détecter pays grâce aux noms de domaines mail\n",
    "import re\n",
    "for chemin in liste_articles:\n",
    "    article = lire_fichier(chemin)\n",
    "    chemin = chemin.split(\"/\")[-1]\n",
    "    chemin = chemin.split(\"\\\\\")[-1]\n",
    "    debut = article[:1000]\n",
    "    if chemin not in dico_continent.keys():\n",
    "        #regex trouvée en ligne et adaptée pour prendre en compte adresses type {pinzhen.chen, n.bogoych}@ed.ac.uk\n",
    "        emails = re.findall(\"({?[a-zA-Z0-9_.,\\s+-]+}?@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\", debut)\n",
    "        for mail in emails:\n",
    "            domaine = mail.split(\".\")[-1]\n",
    "            #tricher pour les domaines qui correspondent pas à leur iso\n",
    "            for iso,dico in countries.items():\n",
    "                if iso.lower()==domaine:\n",
    "                    #print(domaine)\n",
    "                    if chemin not in dico_continent:\n",
    "                        dico_continent[chemin]={}\n",
    "                    dico_continent[chemin][domaine]=dico[\"continent\"]\n",
    "print(len(dico_continent))\n",
    "\n",
    "ecrire_json(\"dico_continentv06-2.json\",dico_continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc615a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dico forme {url:{continent:nb}}\n",
    "countries = ouvrir_json(\"countries_continents.json\")\n",
    "dico_continent = {}\n",
    "for chemin in liste_articles:\n",
    "    article = lire_fichier(chemin)\n",
    "    chemin = chemin.split(\"/\")[-1]\n",
    "    chemin = chemin.split(\"\\\\\")[-1]\n",
    "    debut = article[:500]\n",
    "    for iso, dico in countries.items():\n",
    "        if dico[\"name\"] in debut:\n",
    "            if chemin not in dico_continent:\n",
    "                dico_continent[chemin]={}\n",
    "            if dico[\"continent\"] not in dico_continent[chemin]:\n",
    "                dico_continent[chemin][dico[\"continent\"]]=0\n",
    "            dico_continent[chemin][dico[\"continent\"]]+=1\n",
    "            dico_continent[chemin] = sorted(dico_continent[chemin].items(), key=lambda x:x[1], reverse=True)\n",
    "            dico_continent[chemin] = dict(dico_continent[chemin])\n",
    "print(dico_continent)\n",
    "ecrire_json(\"dico_continent_cptev06-2.json\",dico_continent)\n",
    "\n",
    "#dico triés à l'intérieur donc faut juste prendre la première valeur du dico interne pour avoir le continent maj\n",
    "dico_continent = ouvrir_json(\"dico_continentv06-2.json\")\n",
    "comparaison_claims = ouvrir_json('stopwords/comparaison_claims_chiffres_equilibre_sw_corr.json')\n",
    "\n",
    "dico_continent_claims = {}\n",
    "for url,dico in dico_continent.items():\n",
    "    if url in comparaison_claims and \".cite\" not in url:\n",
    "        dico_continent_claims[url]=list(dico_continent[url].items())[0]\n",
    "\n",
    "ecrire_json(\"dico_continent_claims_sw_corr.json\",dico_continent_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faire une liste par continent contenant les chemins des articles correspondants\n",
    "liste_as, liste_af, liste_eu, liste_na, liste_sa, liste_oc = [],[],[],[],[],[]\n",
    " #liste_continents=[\"AS\",\"AF\",\"EU\",\"NA\",\"SA\",\"OC\"]\n",
    "for url, tuplee in dico_continent_claims.items():\n",
    "    #for c in liste_continents:\n",
    "        if tuplee[1] == \"AS\":\n",
    "            liste_as.append(url)\n",
    "        if tuplee[1] == \"AF\":\n",
    "            liste_af.append(url)\n",
    "        if tuplee[1] == \"EU\":\n",
    "            liste_eu.append(url)\n",
    "        if tuplee[1] == \"NA\":\n",
    "            liste_na.append(url)\n",
    "        if tuplee[1] == \"SA\":\n",
    "            liste_sa.append(url)\n",
    "        if tuplee[1] == \"OC\":\n",
    "            liste_oc.append(url)\n",
    "            \n",
    "print(len(liste_as),len(liste_af),len(liste_eu),len(liste_na),len(liste_sa),len(liste_oc))\n",
    "ecrire_json(\"liste_chemins_as_sw_corr.json\",liste_as)\n",
    "ecrire_json(\"liste_chemins_af_sw_corr.json\",liste_af)\n",
    "ecrire_json(\"liste_chemins_eu_sw_corr.json\",liste_eu)\n",
    "ecrire_json(\"liste_chemins_na_sw_corr.json\",liste_na)\n",
    "ecrire_json(\"liste_chemins_sa_sw_corr.json\",liste_sa)\n",
    "ecrire_json(\"liste_chemins_oc_sw_corr.json\",liste_oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b205c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#associer les articles et leur continent aux degrés de claims correspondants\n",
    "dico_strengths = ouvrir_json(\"stopwords/comparaison_claims_chiffres_equilibre_sw_corr.json\")\n",
    "dico_cluster_as, dico_cluster_af, dico_cluster_eu, dico_cluster_na, dico_cluster_sa, dico_cluster_oc = {},{},{},{},{},{}\n",
    "liste_url_as = ouvrir_json(\"liste_chemins_as_sw_corr.json\")\n",
    "liste_url_af = ouvrir_json(\"liste_chemins_af_sw_corr.json\")\n",
    "liste_url_eu = ouvrir_json(\"liste_chemins_eu_sw_corr.json\")\n",
    "liste_url_na = ouvrir_json(\"liste_chemins_na_sw_corr.json\")\n",
    "liste_url_sa = ouvrir_json(\"liste_chemins_sa_sw_corr.json\")\n",
    "liste_url_oc = ouvrir_json(\"liste_chemins_oc_sw_corr.json\")\n",
    "print(len(liste_url_as),len(liste_url_af),len(liste_url_eu),len(liste_url_na),len(liste_url_sa),len(liste_url_oc))\n",
    "for chemin in dico_strengths.keys():\n",
    "    if chemin in liste_url_as:\n",
    "        if chemin not in dico_cluster_as:\n",
    "            dico_cluster_as[chemin] = dico_strengths[chemin]\n",
    "        \n",
    "    if chemin in liste_url_af:\n",
    "        if chemin not in dico_cluster_af:\n",
    "            dico_cluster_af[chemin] = dico_strengths[chemin]\n",
    "            \n",
    "    if chemin in liste_url_eu:\n",
    "        if chemin not in dico_cluster_eu:\n",
    "            dico_cluster_eu[chemin] = dico_strengths[chemin]\n",
    "            \n",
    "    if chemin in liste_url_na:\n",
    "        if chemin not in dico_cluster_na:\n",
    "            dico_cluster_na[chemin] = dico_strengths[chemin]\n",
    "            \n",
    "    if chemin in liste_url_sa:\n",
    "        if chemin not in dico_cluster_sa:\n",
    "            dico_cluster_sa[chemin] = dico_strengths[chemin]\n",
    "            \n",
    "    if chemin in liste_url_oc:\n",
    "        if chemin not in dico_cluster_oc:\n",
    "            dico_cluster_oc[chemin] = dico_strengths[chemin]\n",
    "        \n",
    "print(len(dico_cluster_as))\n",
    "ecrire_json(\"comparaison_claims_as_sw_corr.json\",dico_cluster_as)\n",
    "print(len(dico_cluster_af))\n",
    "ecrire_json(\"comparaison_claims_af_sw_corr.json\",dico_cluster_af)\n",
    "print(len(dico_cluster_eu))\n",
    "ecrire_json(\"comparaison_claims_eu_sw_corr.json\",dico_cluster_eu)\n",
    "print(len(dico_cluster_na))\n",
    "ecrire_json(\"comparaison_claims_na_sw_corr.json\",dico_cluster_na)\n",
    "print(len(dico_cluster_sa))\n",
    "ecrire_json(\"comparaison_claims_sa_sw_corr.json\",dico_cluster_sa)\n",
    "print(len(dico_cluster_oc))\n",
    "ecrire_json(\"comparaison_claims_oc_sw_corr.json\",dico_cluster_oc)\n",
    "\n",
    "\n",
    "# même idée mais on ne s'intéresse pas aux articles individuellement, mais au nombre de claims liés à chaque continent par partie\n",
    "cont = [\"as\",\"af\",\"eu\",\"na\",\"sa\",\"oc\"]\n",
    "for c in cont:\n",
    "    dico_partiemaj = ouvrir_json(\"comparaison_claims_%s_sw_corr.json\"%c)\n",
    "    dico_compte_partiemaj = {\"ccl\":0,\"abstract\":0,\"intro\":0, \"corps\":0}\n",
    "    for article, liste in dico_partiemaj.items():\n",
    "        for i in range(len(liste)):\n",
    "            if liste[i][0] == \"ccl\":\n",
    "                dico_compte_partiemaj[\"ccl\"]+=1\n",
    "            if liste[i][0] == \"abstract\":\n",
    "                dico_compte_partiemaj[\"abstract\"]+=1\n",
    "            if liste[i][0] == \"intro\":\n",
    "                dico_compte_partiemaj[\"intro\"]+=1\n",
    "            if liste[i][0] == \"corps\":\n",
    "                dico_compte_partiemaj[\"corps\"]+=1\n",
    "    print(dico_compte_partiemaj)\n",
    "    #print(\"=\"*70)\n",
    "    ecrire_json(\"dico_total_%s_sw_tt_corr.json\"%c,dico_compte_partiemaj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prendre en compte les années pour voir l'évolution de la présences des continents dans le temps\n",
    "#dico_nb_genre_url = ouvrir_json(\"dico_nb_genre_urlv06-2.json\")\n",
    "dico_annees = {}\n",
    "[dico_annees.setdefault(i, {}) for i in range(1979,2021)] \n",
    "continents = [\"as\",\"af\",\"eu\",\"na\",\"sa\",\"oc\"]\n",
    "for conti in continents:\n",
    "    dico_nb_url = ouvrir_json(\"comparaison_claims_%s_sw_corr.json\"%conti)\n",
    "    for url, genre in dico_nb_url.items():\n",
    "        #on traite les articles de 2020 à part car nommage des fichiers différent\n",
    "        if conti not in dico_annees[2020]:\n",
    "            dico_annees[2020][conti]=0\n",
    "        if \"2020.acl\" in url:\n",
    "            dico_annees[2020][conti]+=1\n",
    "\n",
    "        for an in range(1979,2020):\n",
    "            if conti not in dico_annees[an]:\n",
    "                dico_annees[an][conti]=0\n",
    "            if \"P\"+str(an)[-2:] in url:\n",
    "                dico_annees[an][conti]+=1\n",
    "        #print(url)\n",
    "print(dico_annees)\n",
    "ecrire_json(\"evol_nb_conti_corr.json\",dico_annees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caebdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb autrices par continents (sans année)\n",
    "continents = [\"as\",\"af\",\"eu\",\"na\",\"sa\",\"oc\"]\n",
    "dico_autrice_conti = {}\n",
    "[dico_autrice_conti.setdefault(i, []) for i in continents] \n",
    "dico_nb_genre_url = ouvrir_json(\"dico_nb_genre_urlv06-2.json\")\n",
    "\n",
    "for cont in continents:\n",
    "    nb_art_h = 0\n",
    "    nb_art_f = 0\n",
    "    nb_art_e = 0\n",
    "    dico_nb_url = ouvrir_json(\"comparaison_claims_%s_sw_corr.json\"%cont)\n",
    "    for url, conti in dico_nb_url.items():\n",
    "        url_p = url.split(\"/\")[-1]\n",
    "        url_p = url_p.split(\".\")[:-1]\n",
    "        url_p = (\".\").join(url_p)\n",
    "        for url_np,genre in dico_nb_genre_url.items():\n",
    "            if url_p in url_np:\n",
    "                if genre==\"h\":\n",
    "                    nb_art_h+=1\n",
    "                elif genre==\"f\":\n",
    "                    nb_art_f+=1\n",
    "                    #print(url)\n",
    "                else:\n",
    "                    nb_art_e+=1\n",
    "            dico_autrice_conti[cont]=[nb_art_h,nb_art_f,nb_art_e]\n",
    "\n",
    "        #print(url)\n",
    "print(dico_autrice_conti)\n",
    "ecrire_json(\"evol_nb_autrices_conti_corr.json\",dico_autrice_conti)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
